{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Progetto Analisi Dati (2021/22)\n",
        "Giuseppe Bruno (579265)"
      ],
      "metadata": {
        "id": "cHL_BpvCPAiY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#link per il download del dataset di test\n",
        "link_test=''"
      ],
      "metadata": {
        "id": "X6KwejhdPK-U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nel seguito vengono riportati i migliori due modelli ottenuti dall'analisi. Il primo è un SVC, il secondo un \"ibrido\" FCN+SVC.\n",
        "\n",
        "(I link presenti nel resto del codice sono diretti ai file dei modelli già allenati, così da non dover ripetere la procedura, che richiederebbe almeno 4 ore. Per completezza è comunque riportato in fondo il codice per effettuare nuovamente il training.)\n"
      ],
      "metadata": {
        "id": "Aw-zIb86PcO_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modello 1 (SVC)"
      ],
      "metadata": {
        "id": "2WMs-6M_O3jU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9JONMJPGaetO"
      },
      "outputs": [],
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import joblib\n",
        "import pickle\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, cohen_kappa_score, balanced_accuracy_score\n",
        "\n",
        "# Autenticazione\n",
        "print(\"1) Autenticazione\")\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "#Download dei dati\n",
        "print(\"2) Download dei dati\")\n",
        "#link_test = 'https://drive.google.com/file/d/1iwVcXgCJMUARL0pmdASFRTC8vJTHZF/view?usp=sharing'\n",
        "link=link_test\n",
        "id = link.split(\"/\")[-2]\n",
        "downloaded = drive.CreateFile({'id':id})\n",
        "downloaded.GetContentFile('data.csv')\n",
        "df = pd.read_csv('data.csv')\n",
        "\n",
        "print(\"3) Download del modello\")\n",
        "link = \"https://drive.google.com/file/d/11h8P8L4tpm9ztEiOMbYzJWvAoXofIhWx/view?usp=sharing\"\n",
        "id = link.split(\"/\")[-2]\n",
        "downloaded = drive.CreateFile({'id':id})\n",
        "downloaded.GetContentFile('svc.sav')\n",
        "clf=pickle.load(open(\"svc.sav\", 'rb'))\n",
        "\n",
        "link = 'https://drive.google.com/file/d/11fRpMfM9bcxvVIFB9hql35OYGutuH_tW/view?usp=sharing'\n",
        "id = link.split(\"/\")[-2]\n",
        "downloaded = drive.CreateFile({'id':id})\n",
        "downloaded.GetContentFile('scaler.sav')\n",
        "scaler=pickle.load(open(\"scaler.sav\", 'rb'))\n",
        "\n",
        "# Separazione di X e Y\n",
        "Y=df[\"V1\"].values\n",
        "X=df.drop([\"V1\"],axis=1).values\n",
        "\n",
        "# Standardizzazione di X\n",
        "n_istances, n_timesteps= X.shape\n",
        "X=np.reshape(X, (-1,1))\n",
        "X=scaler.transform(X)\n",
        "X=np.reshape(X, (n_istances, n_timesteps))\n",
        "\n",
        "print(\"4) Predizione (in fase di sperimentazione ha richiesto circa 10 minuti)\")\n",
        "Y_pred=clf.predict(X)\n",
        "\n",
        "print(\"5) Valutazione:\")\n",
        "print(\" - Accuracy: \", accuracy_score(Y,Y_pred))\n",
        "print(\" - K: \", cohen_kappa_score( Y,Y_pred))\n",
        "print(\" - Balanced Accuracy: \", balanced_accuracy_score(Y, Y_pred))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modello 2 (FCN+SVM)"
      ],
      "metadata": {
        "id": "9OLAvIlQKDBF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import joblib\n",
        "import pickle\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, cohen_kappa_score, balanced_accuracy_score\n",
        "\n",
        "from keras.utils import np_utils\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.layers import Conv1D, MaxPooling1D, Flatten, Dropout, Dense\n",
        "from keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "\n",
        "# Autenticazione\n",
        "print(\"1) Autenticazione\")\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "\n",
        "\n",
        "#Download dei dati\n",
        "print(\"2) Download dei dati\")\n",
        "#link = 'https://drive.google.com/file/d/1iwVcXgCJMUARL0pmdASFRAYTC8vJTHZF/view?usp=sharing'\n",
        "link= link_test\n",
        "id = link.split(\"/\")[-2]\n",
        "downloaded = drive.CreateFile({'id':id})\n",
        "downloaded.GetContentFile('data.csv')\n",
        "df = pd.read_csv('data.csv')\n",
        "\n",
        "\n",
        "#Download dei modelli\n",
        "print(\"3) Importazione dei modelli\")\n",
        "#SVC\n",
        "link = \"https://drive.google.com/file/d/14SVESXejr8_wad3qVtEV_8agawrAvbtz/view?usp=sharing\"\n",
        "id = link.split(\"/\")[-2]\n",
        "downloaded = drive.CreateFile({'id':id})\n",
        "downloaded.GetContentFile('svc_prob_all.sav')\n",
        "model_svc=pickle.load(open(\"svc_prob_all.sav\", 'rb'))\n",
        "\n",
        "#Scaler\n",
        "link = 'https://drive.google.com/file/d/11fRpMfM9bcxvVIFB9hql35OYGutuH_tW/view?usp=sharing'\n",
        "id = link.split(\"/\")[-2]\n",
        "downloaded = drive.CreateFile({'id':id})\n",
        "downloaded.GetContentFile('scaler.sav')\n",
        "scaler=pickle.load(open(\"scaler.sav\", 'rb'))\n",
        "\n",
        "#Econder\n",
        "link = \"https://drive.google.com/file/d/1ESp3Tdgb_SmBbvJkKp7Mc5W__N4TWuWr/view?usp=sharing\"\n",
        "id = link.split(\"/\")[-2]\n",
        "downloaded = drive.CreateFile({'id':id})\n",
        "downloaded.GetContentFile('encoder.sav')\n",
        "encoder=pickle.load(open(\"encoder.sav\", 'rb'))\n",
        "\n",
        "#FCN\n",
        "#link = 'https://drive.google.com/file/d/11iDyvmqKYw7t2ZCecHuEFlfRJKV5i4RE/view?usp=sharing'\n",
        "link= 'https://drive.google.com/file/d/1GXMDuU13cPiHht5rf1_tTEQhTiCEYKaW/view?usp=sharing'\n",
        "id = link.split(\"/\")[-2]\n",
        "downloaded = drive.CreateFile({'id':id})\n",
        "downloaded.GetContentFile('mdl_wts_8542.hdf5')\n",
        "\n",
        "model_fcn = Sequential()\n",
        "model_fcn.add(Conv1D(filters=256, kernel_size=24, activation='relu', input_shape=(46,1)))\n",
        "model_fcn.add(Conv1D(filters=512, kernel_size=12, activation='relu'))\n",
        "model_fcn.add(Conv1D(filters=256, kernel_size=3, activation='relu'))\n",
        "model_fcn.add(Dropout(0.5))\n",
        "model_fcn.add(MaxPooling1D(1))\n",
        "model_fcn.add(Flatten())\n",
        "model_fcn.add(Dense(1024, activation='relu'))\n",
        "model_fcn.add(Dense(18, activation='softmax'))\n",
        "optimizer = Adam(learning_rate=0.001, decay=0.0)\n",
        "model_fcn.compile(loss='categorical_crossentropy', optimizer=optimizer , metrics=['accuracy'])\n",
        "#model.summary()\n",
        "model_fcn.load_weights(\"mdl_wts_8542.hdf5\")\n",
        "\n",
        "\n",
        "print(\"4) Preprocessing\")\n",
        "# Separazione di X e Y\n",
        "Y=df[\"V1\"].values\n",
        "X=df.drop([\"V1\"],axis=1).values\n",
        "\n",
        "# Standardizzazione di X\n",
        "n_istances, n_timesteps= X.shape\n",
        "X=np.reshape(X, (-1,1))\n",
        "X=scaler.transform(X)\n",
        "X=np.reshape(X, (n_istances, n_timesteps))\n",
        "\n",
        "#Econding di Y\n",
        "encoded_Y = encoder.transform(Y)\n",
        "Y_encoded = np_utils.to_categorical(encoded_Y)\n",
        "\n",
        "\n",
        "\n",
        "print(\"5) Predizione (in fase di sperimentazione ha richiesto circa 12 minuti)\")\n",
        "Y_svc=model_svc.predict_proba(X)\n",
        "Y_fcn=model_fcn.predict(X)\n",
        "Y_pred=encoder.inverse_transform(np.argmax(Y_svc+0.9*Y_fcn,axis=1))\n",
        "\n",
        "\n",
        "\n",
        "print(\"6) Valutazione:\")\n",
        "print(\" - Accuracy: \", accuracy_score(Y,Y_pred))\n",
        "print(\" - K: \", cohen_kappa_score( Y,Y_pred))\n",
        "print(\" - Balanced Accuracy: \", balanced_accuracy_score(Y, Y_pred))"
      ],
      "metadata": {
        "id": "TPuOVHJCKCsF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "WcdXDRC8psUh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Primo modello (circa 40 minuti)"
      ],
      "metadata": {
        "id": "F7WhadPEpvx5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, cohen_kappa_score, balanced_accuracy_score\n",
        "\n",
        "# Autenticazione\n",
        "print(\"Autenticazione\")\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "print(\"Download del training set\")\n",
        "link='https://drive.google.com/file/d/1iwVcXgCJMUARL0pmdASFRAYTC8vJTHZF/view?usp=sharing'\n",
        "id = link.split(\"/\")[-2]\n",
        "downloaded = drive.CreateFile({'id':id})\n",
        "downloaded.GetContentFile('train.csv')\n",
        "df_train = pd.read_csv('train.csv')\n",
        "\n",
        "print(\"Download del test set\")\n",
        "link= link_test\n",
        "id = link.split(\"/\")[-2]\n",
        "downloaded = drive.CreateFile({'id':id})\n",
        "downloaded.GetContentFile('test.csv')\n",
        "df_test = pd.read_csv('test.csv')\n",
        "\n",
        "\n",
        "# Separazione di X e Y\n",
        "Y_train=df_train[\"V1\"].values\n",
        "X_train=df_train.drop([\"V1\"],axis=1).values\n",
        "Y_test=df_test[\"V1\"].values\n",
        "X_test=df_test.drop([\"V1\"],axis=1).values\n",
        "\n",
        "# Standardizzazione dell'intero dataset\n",
        "n_istances, n_timesteps= X_train.shape\n",
        "X_train=np.reshape(X_train, (-1,1))\n",
        "scaler=StandardScaler()\n",
        "X_train=scaler.fit_transform(X_train)\n",
        "X_train=np.reshape(X_train, (n_istances, n_timesteps))\n",
        "\n",
        "n_istances, n_timesteps= X_test.shape\n",
        "X_test=np.reshape(X_test, (-1,1))\n",
        "X_test=scaler.transform(X_test)\n",
        "X_test=np.reshape(X_test, (n_istances, n_timesteps))\n",
        "\n",
        "# Training\n",
        "print(\"Training\")\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "clf_svc=SVC(C=3,gamma=0.2,kernel='rbf')\n",
        "clf_svc.fit(X_train,Y_train)\n",
        "print(\"Training concluso\")\n",
        "\n",
        "# Predizione sul test set\n",
        "print(\"Predizione sul test set\")\n",
        "Y_pred=clf_svc.predict(X_test)\n",
        "\n",
        "\n",
        "print(\"Valutazione:\")\n",
        "print(\" - Accuracy: \", accuracy_score(Y_test,Y_pred))\n",
        "print(\" - K: \", cohen_kappa_score( Y_test,Y_pred))\n",
        "print(\" - Balanced Accuracy: \", balanced_accuracy_score(Y_test, Y_pred))"
      ],
      "metadata": {
        "id": "yCtA4wCJpvFe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Secondo modello (circa 4 ore). Per il training della FCN è necessario attivare la GPU."
      ],
      "metadata": {
        "id": "slvmyM-crHV0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, cohen_kappa_score, balanced_accuracy_score\n",
        "\n",
        "# Autenticazione\n",
        "print(\"Autenticazione\")\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "print(\"Download del training set\")\n",
        "link='https://drive.google.com/file/d/1iwVcXgCJMUARL0pmdASFRAYTC8vJTHZF/view?usp=sharing'\n",
        "id = link.split(\"/\")[-2]\n",
        "downloaded = drive.CreateFile({'id':id})\n",
        "downloaded.GetContentFile('train.csv')\n",
        "df_train = pd.read_csv('train.csv')\n",
        "\n",
        "print(\"Download del test set\")\n",
        "link= link_test\n",
        "id = link.split(\"/\")[-2]\n",
        "downloaded = drive.CreateFile({'id':id})\n",
        "downloaded.GetContentFile('test.csv')\n",
        "df_test = pd.read_csv('test.csv')\n",
        "\n",
        "\n",
        "# Separazione di X e Y\n",
        "Y_train=df_train[\"V1\"].values\n",
        "X_train=df_train.drop([\"V1\"],axis=1).values\n",
        "Y_test=df_test[\"V1\"].values\n",
        "X_test=df_test.drop([\"V1\"],axis=1).values\n",
        "\n",
        "# Standardizzazione dell'intero dataset\n",
        "n_istances, n_timesteps= X_train.shape\n",
        "X_train=np.reshape(X_train, (-1,1))\n",
        "scaler=StandardScaler()\n",
        "X_train=scaler.fit_transform(X_train)\n",
        "X_train=np.reshape(X_train, (n_istances, n_timesteps))\n",
        "\n",
        "n_istances, n_timesteps= X_test.shape\n",
        "X_test=np.reshape(X_test, (-1,1))\n",
        "X_test=scaler.transform(X_test)\n",
        "X_test=np.reshape(X_test, (n_istances, n_timesteps))\n",
        "\n",
        "# Training SVC\n",
        "print(\"Training del SVC\")\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "clf_svc=SVC(C=3,gamma=0.2,kernel='rbf', probability=True)\n",
        "clf_svc.fit(X_train,Y_train)\n",
        "print(\"Training del SVC concluso\")\n",
        "\n",
        "\n",
        "#Training FCN\n",
        "print(\"Training della FCN\")\n",
        "from keras.utils import np_utils\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.layers import Conv1D, MaxPooling1D, Flatten, Dropout, Dense\n",
        "from keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.callbacks import  EarlyStopping, ReduceLROnPlateau\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "model_fcn = Sequential()\n",
        "model_fcn.add(Conv1D(filters=256, kernel_size=24, activation='relu', input_shape=(46,1)))\n",
        "model_fcn.add(Conv1D(filters=512, kernel_size=12, activation='relu'))\n",
        "model_fcn.add(Conv1D(filters=256, kernel_size=3, activation='relu'))\n",
        "model_fcn.add(Dropout(0.5))\n",
        "model_fcn.add(MaxPooling1D(1))\n",
        "model_fcn.add(Flatten())\n",
        "model_fcn.add(Dense(1024, activation='relu'))\n",
        "model_fcn.add(Dense(18, activation='softmax'))\n",
        "optimizer = Adam(learning_rate=0.001, decay=0.0)\n",
        "model_fcn.compile(loss='categorical_crossentropy', optimizer=optimizer , metrics=['accuracy'])\n",
        "#model.summary()\n",
        "\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(Y_train)\n",
        "encoded_Y_train = encoder.transform(Y_train)\n",
        "Y_train_enc = np_utils.to_categorical(encoded_Y_train)\n",
        "\n",
        "encoded_Y_test = encoder.transform(Y_test)\n",
        "Y_test_enc = np_utils.to_categorical(encoded_Y_test)\n",
        "\n",
        "X_train2, X_val, Y_train2, Y_val = train_test_split(X_train, Y_train_enc, test_size = 0.1, stratify=Y_train,random_state = 0)\n",
        "\n",
        "\n",
        "\n",
        "earlyStopping = EarlyStopping(monitor='val_accuracy', patience=15, verbose=0, mode='max')\n",
        "mcp_save = ModelCheckpoint('mdl_wts_full.hdf5', save_best_only=True, monitor='val_accuracy', mode='max')\n",
        "reduce_lr_loss = ReduceLROnPlateau(monitor='val_accuracy', factor=0.1, patience=8, verbose=1,  min_delta=1e-4, mode='max')\n",
        "model_fcn.fit(X_train2, Y_train2, batch_size=1024, epochs=150, validation_data=(X_val, Y_val), callbacks=[earlyStopping, reduce_lr_loss])\n",
        "\n",
        "\n",
        "# Predizione sul test set\n",
        "print(\"Predizione sul test set\")\n",
        "Y_svc=model_svc.predict_proba(X_test)\n",
        "Y_fcn=model_fcn.predict(X_test)\n",
        "Y_pred=encoder.inverse_transform(np.argmax(Y_svc+Y_fcn,axis=1))\n",
        "\n",
        "\n",
        "print(\"Valutazione:\")\n",
        "print(\" - Accuracy: \", accuracy_score(Y_test,Y_pred))\n",
        "print(\" - K: \", cohen_kappa_score(Y_test,Y_pred))\n",
        "print(\" - Balanced Accuracy: \", balanced_accuracy_score(Y_test, Y_pred))\n"
      ],
      "metadata": {
        "id": "T7TAIptArGeg"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}